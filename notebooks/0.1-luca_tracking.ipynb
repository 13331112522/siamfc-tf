{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using Tensorflow 1.1.0\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "print('Using Tensorflow '+tf.__version__)\n",
    "assert tf.__version__>='1.0.0', ('You should use Tensorflow 1.0 or superior')\n",
    "#import matplotlib\n",
    "import matplotlib.patches as patches\n",
    "import matplotlib.pyplot as plt\n",
    "#matplotlib.use('Agg')\n",
    "#%matplotlib inline\n",
    "import sys\n",
    "sys.path.append('../')\n",
    "import os.path\n",
    "import csv\n",
    "import numpy as np\n",
    "\n",
    "from src.argparse import argparse\n",
    "from src.region_to_bbox import region_to_bbox\n",
    "from src.pprint_params import pprint_params\n",
    "from src.Tracker import Tracker\n",
    "from src.crops import *\n",
    "from src.siamese import import_siamese"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "#### SETTINGS\n",
      "\n",
      "## tracker hyperparams\n",
      "hp(scale_min=0.2, window_influence=0.25, z_lr=0, scale_max=5, scale_step=1.04, scale_num=3, scale_penalty=0.97, response_up=8, scale_lr=0.59)\n",
      "\n",
      "## evaluation\n",
      "evaluation(start_frame=0, TRE_th_points=101, dataset=u'validation', video='vot2016_ball1', TRE_subseq=3, stop_on_failure=0)\n",
      "\n",
      "## run\n",
      "run(debug=0, visualization=0, gpus=[1])\n",
      "\n",
      "## environment\n",
      "env(root_pretrained=u'../pretrained', root_parameters=u'../parameters', root_dataset=u'../data')\n",
      "\n",
      "## design\n",
      "design(exemplar_sz=127, net_gray=u'', search_sz=255, join_method=u'xcorr', tot_stride=4, context=0.5, net=u'baseline-conv5_e55.mat', windowing=u'cosine_sum', score_sz=33)\n"
     ]
    }
   ],
   "source": [
    "hp = {\"z_lr\":0}\n",
    "evaluation = {\"video\": \"vot2016_ball1\"}\n",
    "run = {\"visualization\":0,\"debug\":0}\n",
    "hp,evaluation,run,env,design = argparse(hp, evaluation, run)\n",
    "pprint_params((hp,evaluation,run,env,design),['tracker hyperparams','evaluation','run','environment','design'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "__init__() takes exactly 3 arguments (6 given)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-3-c3bd7a0de41b>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     10\u001b[0m \u001b[0mbboxes\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mzeros\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnum_frames\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m4\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;31m# stores tracker's output for evaluation\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     11\u001b[0m \u001b[0;31m# create an instance of the class Tracker and initialize it with groundtruth\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 12\u001b[0;31m \u001b[0mtracker\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mTracker\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minit_bbox\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mevaluation\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstart_frame\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdesign\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvideo_folder\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mframe_name_list\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     13\u001b[0m \u001b[0;31m# used to pad the crops\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     14\u001b[0m \u001b[0mavg_chan\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmean\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtracker\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mframe\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maxis\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mTypeError\u001b[0m: __init__() takes exactly 3 arguments (6 given)"
     ]
    }
   ],
   "source": [
    "video_folder = os.path.join(env.root_dataset, evaluation.dataset, evaluation.video)\n",
    "gt_file = os.path.join(video_folder, 'groundtruth.txt')\n",
    "gt = np.genfromtxt(gt_file, delimiter=',')\n",
    "# bbox is in format <cx,cy,w,h>\n",
    "init_bbox = region_to_bbox(gt[evaluation.start_frame])\n",
    "frame_name_list = [f for f in os.listdir(video_folder) if f.endswith(\".jpg\")]\n",
    "frame_name_list.sort()\n",
    "assert len(gt) == len(frame_name_list), ('Number of frames and number of GT lines should be equal.')\n",
    "num_frames = len(gt)\n",
    "bboxes = np.zeros((num_frames,4)) # stores tracker's output for evaluation\n",
    "# create an instance of the class Tracker and initialize it with groundtruth\n",
    "tracker = Tracker(init_bbox, evaluation.start_frame, design, video_folder, frame_name_list)\n",
    "# used to pad the crops\n",
    "avg_chan = np.mean(tracker.frame, axis=(0,1))\n",
    "# fixed multiplicative factors to scale up/down the target\n",
    "scale_factors = hp.scale_step**np.linspace(-np.ceil(hp.scale_num/2), np.ceil(hp.scale_num/2), hp.scale_num)\n",
    "scaled_exemplar = scale_factors * tracker.z_sz\n",
    "min_z = hp.scale_min * tracker.z_sz\n",
    "max_z = hp.scale_max * tracker.z_sz\n",
    "min_x = hp.scale_min * tracker.x_sz\n",
    "max_x = hp.scale_max * tracker.x_sz"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'tracker' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-4-14dc66b16529>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mtracker\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mframe_padded\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtracker\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnpad\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpad_frame\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtracker\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mframe\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtracker\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbbox\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpos\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtracker\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mz_sz\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mavg_chan\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m;\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0;31m# template of the target, will be fed to the graph\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0mz_crops\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mextract_crops\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtracker\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mframe_padded\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtracker\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnpad\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtracker\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbbox\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpos\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtracker\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mz_sz\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdesign\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexemplar_sz\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'tracker' is not defined"
     ]
    }
   ],
   "source": [
    "tracker.frame_padded, tracker.npad = pad_frame(tracker.frame, tracker.bbox.pos, tracker.z_sz, avg_chan);\n",
    "# template of the target, will be fed to the graph\n",
    "z_crops = extract_crops(tracker.frame_padded, tracker.npad, tracker.bbox.pos, tracker.z_sz, design.exemplar_sz)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# cosine window to penalize large displacements\n",
    "hann_1d = np.expand_dims(np.hanning(design.score_sz*hp.response_up), axis=0)\n",
    "penalty = np.transpose(hann_1d) * hann_1d\n",
    "penalty = penalty / np.sum(penalty)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "> Layer 1\n",
      "\t\tCONV: setting br_conv1f br_conv1b\n",
      "\t\tCONV: stride 2, filter-group False\n",
      "\t\tBNORM: setting br_bn1b br_bn1m br_bn1x\n",
      "\t\tMAX-POOL: size 3 and stride 2\n",
      "> Layer 2\n",
      "\t\tCONV: setting br_conv2f br_conv2b\n",
      "\t\tCONV: stride 1, filter-group True\n",
      "\t\tBNORM: setting br_bn2b br_bn2m br_bn2x\n",
      "\t\tMAX-POOL: size 3 and stride 1\n",
      "> Layer 3\n",
      "\t\tCONV: setting br_conv3f br_conv3b\n",
      "\t\tCONV: stride 1, filter-group False\n",
      "\t\tBNORM: setting br_bn3b br_bn3m br_bn3x\n",
      "> Layer 4\n",
      "\t\tCONV: setting br_conv4f br_conv4b\n",
      "\t\tCONV: stride 1, filter-group True\n",
      "\t\tBNORM: setting br_bn4b br_bn4m br_bn4x\n",
      "> Layer 5\n",
      "\t\tCONV: setting br_conv5f br_conv5b\n",
      "\t\tCONV: stride 1, filter-group True\n"
     ]
    }
   ],
   "source": [
    "# import pre-trained siamese\n",
    "net_path=os.path.join(env.root_pretrained,design.net)\n",
    "X = tf.placeholder(tf.float32)\n",
    "Z = tf.placeholder(tf.float32)\n",
    "net = import_siamese(net_path, X, Z)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Frame 1/105\n"
     ]
    },
    {
     "ename": "TypeError",
     "evalue": "The value of a feed cannot be a tf.Tensor object. Acceptable feed values include Python scalars, strings, lists, numpy ndarrays, or TensorHandles.",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-7-f9ba5fe3a466>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     10\u001b[0m \u001b[0;31m#         test_writer = tf.summary.FileWriter('./logs', sess.graph)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     11\u001b[0m         \u001b[0msess\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrun\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mglobal_variables_initializer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 12\u001b[0;31m         \u001b[0mout\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msess\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrun\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnet\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfeed_dict\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m{\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0mx_crops\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mZ\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0mz_crops\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     13\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     14\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mrun\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvisualization\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/home/berti/tensorflow/local/lib/python2.7/site-packages/tensorflow/python/client/session.pyc\u001b[0m in \u001b[0;36mrun\u001b[0;34m(self, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m    776\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    777\u001b[0m       result = self._run(None, fetches, feed_dict, options_ptr,\n\u001b[0;32m--> 778\u001b[0;31m                          run_metadata_ptr)\n\u001b[0m\u001b[1;32m    779\u001b[0m       \u001b[0;32mif\u001b[0m \u001b[0mrun_metadata\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    780\u001b[0m         \u001b[0mproto_data\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf_session\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTF_GetBuffer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrun_metadata_ptr\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/home/berti/tensorflow/local/lib/python2.7/site-packages/tensorflow/python/client/session.pyc\u001b[0m in \u001b[0;36m_run\u001b[0;34m(self, handle, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m    934\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    935\u001b[0m           \u001b[0;32mif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msubfeed_val\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mops\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTensor\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 936\u001b[0;31m             raise TypeError('The value of a feed cannot be a tf.Tensor object. '\n\u001b[0m\u001b[1;32m    937\u001b[0m                             \u001b[0;34m'Acceptable feed values include Python scalars, '\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    938\u001b[0m                             'strings, lists, numpy ndarrays, or TensorHandles.')\n",
      "\u001b[0;31mTypeError\u001b[0m: The value of a feed cannot be a tf.Tensor object. Acceptable feed values include Python scalars, strings, lists, numpy ndarrays, or TensorHandles."
     ]
    }
   ],
   "source": [
    "# > store initial bbox\n",
    "for i in xrange(evaluation.start_frame+1, 2):\n",
    "    print 'Frame '+str(i)+'/'+str(num_frames)\n",
    "    # call tracker object to read new img\n",
    "    tracker.get_frame(i, video_folder, frame_name_list)    \n",
    "    scaled_search_area = tracker.x_sz * scale_factors\n",
    "    tracker.frame_padded, tracker.npad = pad_frame(tracker.frame, tracker.bbox.pos, scaled_search_area[-1], avg_chan);\n",
    "    x_crops = extract_crops(tracker.frame_padded, tracker.npad, tracker.bbox.pos, scaled_search_area, design.search_sz)\n",
    "    with tf.Session() as sess:\n",
    "#         test_writer = tf.summary.FileWriter('./logs', sess.graph)\n",
    "        sess.run(tf.global_variables_initializer())\n",
    "        out = sess.run(net, feed_dict={X:x_crops, Z:z_crops})\n",
    "    \n",
    "    if run.visualization:\n",
    "        with tf.Session() as sess:\n",
    "            z = sess.run(z_crop)            \n",
    "            x = sess.run(x_crops)\n",
    "            \n",
    "#         z = np.uint8(np.squeeze(z, axis=0))\n",
    "        z = np.uint8(z[0,:,:,:])\n",
    "        x0 = np.uint8(x[0,:,:,:])\n",
    "        x1 = np.uint8(x[1,:,:,:])\n",
    "        x2 = np.uint8(x[2,:,:,:])\n",
    "        fig = plt.figure(1)\n",
    "        ax = fig.add_subplot(111)\n",
    "        r = patches.Rectangle(\n",
    "            (tracker.bbox.pos[1]-tracker.bbox.target_sz[1]/2, tracker.bbox.pos[0]-tracker.bbox.target_sz[0]/2),\n",
    "            tracker.bbox.target_sz[1], tracker.bbox.target_sz[1],\n",
    "            linewidth=2, edgecolor='r', fill=False)    \n",
    "        ax.imshow(tracker.frame)\n",
    "        ax.add_patch(r)\n",
    "        plt.ion()\n",
    "        plt.show()\n",
    "        plt.pause(0.001)\n",
    "\n",
    "        if run.debug:\n",
    "            fig_dbg = plt.figure(2)\n",
    "            ax1 = fig_dbg.add_subplot(141)\n",
    "            ax2 = fig_dbg.add_subplot(142)\n",
    "            ax3 = fig_dbg.add_subplot(143)\n",
    "            ax4 = fig_dbg.add_subplot(144)\n",
    "            ax1.imshow(z)\n",
    "            ax2.imshow(x0)\n",
    "            ax3.imshow(x1)\n",
    "            ax4.imshow(x2)\n",
    "            plt.ion()\n",
    "            plt.show()\n",
    "            plt.pause(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "plt.close('all')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# # initialize figure and visualize\n",
    "# if run.visualization:    \n",
    "#     fig, ax = plt.subplots(1)\n",
    "#     r = patches.Rectangle(\n",
    "#         (tracker.bbox.cx-tracker.bbox.w/2, tracker.bbox.cy-tracker.bbox.h/2),\n",
    "#         tracker.bbox.w, tracker.bbox.h,\n",
    "#         linewidth=2, edgecolor='r', fill=False)\n",
    "#     ax.imshow(tracker.frame)\n",
    "#     ax.add_patch(r)\n",
    "#     plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "tensorflow",
   "language": "python",
   "name": "tensorflow"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
