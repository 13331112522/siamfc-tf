{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using Tensorflow 1.1.0\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "print('Using Tensorflow '+tf.__version__)\n",
    "assert tf.__version__>='1.0.0', ('You should use Tensorflow 1.0 or superior')\n",
    "import matplotlib;\n",
    "from PIL import Image\n",
    "#matplotlib.use('Agg')\n",
    "import matplotlib.patches as patches\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "import sys\n",
    "sys.path.append('../')\n",
    "import os.path\n",
    "import csv\n",
    "import numpy as np\n",
    "\n",
    "from src.argparse import argparse\n",
    "from src.region_to_bbox import region_to_bbox\n",
    "from src.pprint_params import pprint_params\n",
    "from src.Tracker import Tracker\n",
    "from src.crops import *\n",
    "from src.siamese import import_siamese"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "hp = {\"z_lr\":0}\n",
    "evaluation = {\"video\": \"vot2016_helicopter\"}\n",
    "run = {\"visualization\":1,\"debug\":0}\n",
    "hp,evaluation,run,env,design = argparse(hp, evaluation, run)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "video_folder = os.path.join(env.root_dataset, evaluation.dataset, evaluation.video)\n",
    "frame_name_list = [f for f in os.listdir(video_folder) if f.endswith(\".jpg\")]\n",
    "frame_name_list = [os.path.join(env.root_dataset, evaluation.dataset, evaluation.video, '') + s for s in frame_name_list]\n",
    "frame_name_list.sort()\n",
    "num_frames = np.size(frame_name_list)\n",
    "\n",
    "with Image.open(frame_name_list[0]) as img:\n",
    "    frame_sz = np.asarray(img.size)\n",
    "    frame_sz[1], frame_sz[0] = frame_sz[0], frame_sz[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "gt_file = os.path.join(video_folder, 'groundtruth.txt')\n",
    "gt = np.genfromtxt(gt_file, delimiter=',')\n",
    "assert len(gt) == len(frame_name_list), ('Number of frames and number of GT lines should be equal.')\n",
    "# bbox is in format <cx,cy,w,h>\n",
    "init_bbox = region_to_bbox(gt[evaluation.start_frame])\n",
    "bboxes = np.zeros((num_frames,4)) # stores tracker's output for evaluation\n",
    "# create an instance of the class Tracker and initialize it with groundtruth\n",
    "tracker = Tracker(init_bbox, design)\n",
    "# fixed multiplicative factors to scale up/down the target\n",
    "scale_factors = hp.scale_step**np.linspace(-np.ceil(hp.scale_num/2), np.ceil(hp.scale_num/2), hp.scale_num)\n",
    "scaled_exemplar = scale_factors * tracker.z_sz\n",
    "min_z = hp.scale_min * tracker.z_sz\n",
    "max_z = hp.scale_max * tracker.z_sz\n",
    "min_x = hp.scale_min * tracker.x_sz\n",
    "max_x = hp.scale_max * tracker.x_sz"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "#### BUILD THE TF GRAPH #####\n",
    "\n",
    "# Make a queue of file names\n",
    "filename_queue = tf.train.string_input_producer(frame_name_list, shuffle=False, capacity=num_frames)\n",
    "\n",
    "image_reader = tf.WholeFileReader()\n",
    "# Read a whole file from the queue\n",
    "_, image_file = image_reader.read(filename_queue)\n",
    "\n",
    "# Decode the image as a JPEG file, this will turn it into a Tensor\n",
    "image = tf.cast(tf.image.decode_jpeg(image_file), tf.int32)\n",
    "\n",
    "# used to pad the crops\n",
    "avg_chan = tf.cast(tf.reduce_mean(image, axis=(0,1)), tf.int32)\n",
    "\n",
    "frame_padded, npad = pad_frame(image, frame_sz, tracker.bbox.pos, tracker.z_sz, avg_chan);\n",
    "\n",
    "# z_crops = extract_crops(tracker.frame_padded, tracker.npad, tracker.bbox.pos, tracker.z_sz, design.exemplar_sz)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Error reported to Coordinator: <class 'tensorflow.python.framework.errors_impl.CancelledError'>, Enqueue operation was cancelled\n",
      "\t [[Node: input_producer/input_producer_EnqueueMany = QueueEnqueueManyV2[Tcomponents=[DT_STRING], timeout_ms=-1, _device=\"/job:localhost/replica:0/task:0/cpu:0\"](input_producer, input_producer/Identity)]]\n",
      "\n",
      "Caused by op u'input_producer/input_producer_EnqueueMany', defined at:\n",
      "  File \"/usr/lib/python2.7/runpy.py\", line 162, in _run_module_as_main\n",
      "    \"__main__\", fname, loader, pkg_name)\n",
      "  File \"/usr/lib/python2.7/runpy.py\", line 72, in _run_code\n",
      "    exec code in run_globals\n",
      "  File \"/usr/local/lib/python2.7/dist-packages/ipykernel_launcher.py\", line 16, in <module>\n",
      "    app.launch_new_instance()\n",
      "  File \"/usr/local/lib/python2.7/dist-packages/traitlets/config/application.py\", line 658, in launch_instance\n",
      "    app.start()\n",
      "  File \"/usr/local/lib/python2.7/dist-packages/ipykernel/kernelapp.py\", line 477, in start\n",
      "    ioloop.IOLoop.instance().start()\n",
      "  File \"/usr/lib/python2.7/dist-packages/zmq/eventloop/ioloop.py\", line 160, in start\n",
      "    super(ZMQIOLoop, self).start()\n",
      "  File \"/usr/local/lib/python2.7/dist-packages/tornado/ioloop.py\", line 888, in start\n",
      "    handler_func(fd_obj, events)\n",
      "  File \"/usr/local/lib/python2.7/dist-packages/tornado/stack_context.py\", line 277, in null_wrapper\n",
      "    return fn(*args, **kwargs)\n",
      "  File \"/usr/lib/python2.7/dist-packages/zmq/eventloop/zmqstream.py\", line 433, in _handle_events\n",
      "    self._handle_recv()\n",
      "  File \"/usr/lib/python2.7/dist-packages/zmq/eventloop/zmqstream.py\", line 465, in _handle_recv\n",
      "    self._run_callback(callback, msg)\n",
      "  File \"/usr/lib/python2.7/dist-packages/zmq/eventloop/zmqstream.py\", line 407, in _run_callback\n",
      "    callback(*args, **kwargs)\n",
      "  File \"/usr/local/lib/python2.7/dist-packages/tornado/stack_context.py\", line 277, in null_wrapper\n",
      "    return fn(*args, **kwargs)\n",
      "  File \"/usr/local/lib/python2.7/dist-packages/ipykernel/kernelbase.py\", line 283, in dispatcher\n",
      "    return self.dispatch_shell(stream, msg)\n",
      "  File \"/usr/local/lib/python2.7/dist-packages/ipykernel/kernelbase.py\", line 235, in dispatch_shell\n",
      "    handler(stream, idents, msg)\n",
      "  File \"/usr/local/lib/python2.7/dist-packages/ipykernel/kernelbase.py\", line 399, in execute_request\n",
      "    user_expressions, allow_stdin)\n",
      "  File \"/usr/local/lib/python2.7/dist-packages/ipykernel/ipkernel.py\", line 196, in do_execute\n",
      "    res = shell.run_cell(code, store_history=store_history, silent=silent)\n",
      "  File \"/usr/local/lib/python2.7/dist-packages/ipykernel/zmqshell.py\", line 533, in run_cell\n",
      "    return super(ZMQInteractiveShell, self).run_cell(*args, **kwargs)\n",
      "  File \"/usr/local/lib/python2.7/dist-packages/IPython/core/interactiveshell.py\", line 2717, in run_cell\n",
      "    interactivity=interactivity, compiler=compiler, result=result)\n",
      "  File \"/usr/local/lib/python2.7/dist-packages/IPython/core/interactiveshell.py\", line 2821, in run_ast_nodes\n",
      "    if self.run_code(code, result):\n",
      "  File \"/usr/local/lib/python2.7/dist-packages/IPython/core/interactiveshell.py\", line 2881, in run_code\n",
      "    exec(code_obj, self.user_global_ns, self.user_ns)\n",
      "  File \"<ipython-input-5-306f93ecee5c>\", line 4, in <module>\n",
      "    filename_queue = tf.train.string_input_producer(frame_name_list, shuffle=False, capacity=num_frames)\n",
      "  File \"/home/berti/tensorflow/local/lib/python2.7/site-packages/tensorflow/python/training/input.py\", line 232, in string_input_producer\n",
      "    cancel_op=cancel_op)\n",
      "  File \"/home/berti/tensorflow/local/lib/python2.7/site-packages/tensorflow/python/training/input.py\", line 164, in input_producer\n",
      "    enq = q.enqueue_many([input_tensor])\n",
      "  File \"/home/berti/tensorflow/local/lib/python2.7/site-packages/tensorflow/python/ops/data_flow_ops.py\", line 367, in enqueue_many\n",
      "    self._queue_ref, vals, name=scope)\n",
      "  File \"/home/berti/tensorflow/local/lib/python2.7/site-packages/tensorflow/python/ops/gen_data_flow_ops.py\", line 1556, in _queue_enqueue_many_v2\n",
      "    name=name)\n",
      "  File \"/home/berti/tensorflow/local/lib/python2.7/site-packages/tensorflow/python/framework/op_def_library.py\", line 768, in apply_op\n",
      "    op_def=op_def)\n",
      "  File \"/home/berti/tensorflow/local/lib/python2.7/site-packages/tensorflow/python/framework/ops.py\", line 2336, in create_op\n",
      "    original_op=self._default_original_op, op_def=op_def)\n",
      "  File \"/home/berti/tensorflow/local/lib/python2.7/site-packages/tensorflow/python/framework/ops.py\", line 1228, in __init__\n",
      "    self._traceback = _extract_stack()\n",
      "\n",
      "CancelledError (see above for traceback): Enqueue operation was cancelled\n",
      "\t [[Node: input_producer/input_producer_EnqueueMany = QueueEnqueueManyV2[Tcomponents=[DT_STRING], timeout_ms=-1, _device=\"/job:localhost/replica:0/task:0/cpu:0\"](input_producer, input_producer/Identity)]]\n",
      "\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "setting an array element with a sequence.",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-6-e3a2d6a42651>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     12\u001b[0m             \u001b[0mfig\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mplt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfigure\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     13\u001b[0m             \u001b[0max\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfig\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0madd_subplot\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m111\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 14\u001b[0;31m             \u001b[0max\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mimshow\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0muint8\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mim_\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     15\u001b[0m             \u001b[0mplt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mion\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     16\u001b[0m             \u001b[0mplt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshow\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mValueError\u001b[0m: setting an array element with a sequence."
     ]
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAXQAAAD/CAYAAADhYy38AAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x7f95f1c529d0>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "with tf.Session() as sess:\n",
    "    tf.global_variables_initializer().run()\n",
    "    # Coordinate the loading of image files.\n",
    "    coord = tf.train.Coordinator()\n",
    "    threads = tf.train.start_queue_runners(coord=coord)\n",
    "    \n",
    "    # Get an image tensor from the queue\n",
    "    for i in xrange(1):\n",
    "        im = sess.run([frame_padded])\n",
    "        if run.visualization:\n",
    "            im_ = np.squeeze(im, axis=0)\n",
    "            fig = plt.figure(1)\n",
    "            ax = fig.add_subplot(111)\n",
    "            ax.imshow(np.uint8(im_))\n",
    "            plt.ion()\n",
    "            plt.show()\n",
    "            plt.pause(0.001)\n",
    "\n",
    "    # Finish off the filename queue coordinator.\n",
    "    coord.request_stop()\n",
    "    coord.join(threads) "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "tensorflow",
   "language": "python",
   "name": "tensorflow"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
