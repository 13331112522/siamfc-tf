{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using Tensorflow 1.1.0\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "print('Using Tensorflow '+tf.__version__)\n",
    "assert tf.__version__>='1.0.0', ('You should use Tensorflow 1.0 or superior')\n",
    "import matplotlib;\n",
    "from PIL import Image\n",
    "matplotlib.use('Agg')\n",
    "import matplotlib.patches as patches\n",
    "import matplotlib.pyplot as plt\n",
    "#%matplotlib inline\n",
    "import sys\n",
    "sys.path.append('../')\n",
    "import os.path\n",
    "import csv\n",
    "import numpy as np\n",
    "\n",
    "from src.argparse import argparse\n",
    "from src.crops import *\n",
    "from src.siamese import import_siamese"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "hp = {\"z_lr\":0}\n",
    "evaluation = {\"video\": \"test\"}\n",
    "run = {\"visualization\":0,\"debug\":0}\n",
    "hp,evaluation,run,env,design = argparse(hp, evaluation, run)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# # will contain list of all frames to be read\n",
    "# x = tf.placeholder(tf.string)\n",
    "# count_num_files = tf.size(x)\n",
    "# filename_queue = tf.train.string_input_producer(x)\n",
    "# reader=tf.WholeFileReader()\n",
    "# key,value=reader.read(filename_queue)\n",
    "# img = tf.image.decode_jpeg(value)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "video_folder = os.path.join(env.root_dataset, evaluation.dataset, evaluation.video)\n",
    "frame_name_list = [f for f in os.listdir(video_folder) if f.endswith(\".jpg\")]\n",
    "frame_name_list.sort()\n",
    "\n",
    "frame_name_list = [os.path.join(env.root_dataset, evaluation.dataset, evaluation.video, '') + s for s in frame_name_list]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# with tf.Session() as sess:\n",
    "#     num_files = sess.run(count_num_files, feed_dict={x:frame_name_list})\n",
    "#     coord = tf.train.Coordinator()\n",
    "#     threads = tf.train.start_queue_runners(coord=coord)    \n",
    "#     print num_files\n",
    "#     for i in range(10):\n",
    "#         image=sess.run(img, feed_dict={x:frame_name_list})\n",
    "#         print(image.shape)\n",
    "#         Image.fromarray(np.asarray(image)).show()\n",
    "#         pause(0.5)    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[array([[[ 45,  39,  39],\n",
      "        [ 44,  38,  38],\n",
      "        [ 44,  38,  38],\n",
      "        ..., \n",
      "        [ 79,  70,  63],\n",
      "        [ 84,  73,  67],\n",
      "        [ 86,  75,  69]],\n",
      "\n",
      "       [[ 45,  36,  37],\n",
      "        [ 42,  37,  34],\n",
      "        [ 43,  34,  35],\n",
      "        ..., \n",
      "        [ 83,  74,  67],\n",
      "        [ 85,  74,  68],\n",
      "        [ 86,  75,  69]],\n",
      "\n",
      "       [[ 70,  60,  59],\n",
      "        [ 68,  60,  57],\n",
      "        [ 67,  57,  56],\n",
      "        ..., \n",
      "        [ 89,  80,  73],\n",
      "        [ 89,  78,  72],\n",
      "        [ 88,  77,  71]],\n",
      "\n",
      "       ..., \n",
      "       [[117, 107, 105],\n",
      "        [116, 106, 104],\n",
      "        [116, 106, 104],\n",
      "        ..., \n",
      "        [100,  92,  90],\n",
      "        [102,  94,  92],\n",
      "        [104,  96,  94]],\n",
      "\n",
      "       [[122, 112, 110],\n",
      "        [122, 112, 110],\n",
      "        [122, 112, 110],\n",
      "        ..., \n",
      "        [113, 105, 103],\n",
      "        [117, 109, 107],\n",
      "        [120, 112, 110]],\n",
      "\n",
      "       [[130, 120, 118],\n",
      "        [130, 120, 118],\n",
      "        [130, 120, 118],\n",
      "        ..., \n",
      "        [131, 123, 121],\n",
      "        [137, 129, 127],\n",
      "        [139, 131, 129]]], dtype=uint8)]\n"
     ]
    }
   ],
   "source": [
    "# Make a queue of file names\n",
    "filename_queue = tf.train.string_input_producer(frame_name_list)\n",
    "\n",
    "image_reader = tf.WholeFileReader()\n",
    "# Read a whole file from the queue\n",
    "_, image_file = image_reader.read(filename_queue)\n",
    "\n",
    "# Decode the image as a JPEG file, this will turn it into a Tensor\n",
    "image = tf.image.decode_jpeg(image_file)\n",
    "\n",
    "with tf.Session() as sess:\n",
    "    tf.global_variables_initializer().run()\n",
    "    # Coordinate the loading of image files.\n",
    "    coord = tf.train.Coordinator()\n",
    "    threads = tf.train.start_queue_runners(coord=coord)\n",
    "\n",
    "    # Get an image tensor and print its value.\n",
    "    image_tensor = sess.run([image])\n",
    "    print(image_tensor)\n",
    "\n",
    "    # Finish off the filename queue coordinator.\n",
    "    coord.request_stop()\n",
    "    coord.join(threads)    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "tensorflow",
   "language": "python",
   "name": "tensorflow"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
